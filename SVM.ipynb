{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM  (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines** are based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/SVMIntro1.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Identify the right hyper-plane (Scenario-1):** Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle.\n",
    "<img src=\"files/svm1.png\" />\n",
    "* You need to remember a thumb rule to identify the right hyper-plane: “Select the hyper-plane which segregates the two classes better”. In this scenario, hyper-plane “B” has excellently performed this job.\n",
    "\n",
    "2.**Identify the right hyper-plane (Scenario-2):** Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane?\n",
    "<img src=\"files/svm2.png\" /> \n",
    "\n",
    "* Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let’s look at the below snapshot:\n",
    "\n",
    "<img src=\"files/svm3.png\"/>\n",
    "\n",
    "* Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.**Identify the right hyper-plane (Scenario-3):** \n",
    "<img src=\"files/svm4.png\"/>\n",
    "Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Can we classify two classes (Scenario-4)?:** Below, I am unable to segregate the two classes using a straight line, as one of star lies in the territory of other(circle) class as an outlier. \n",
    "<img src=\"files/svm5.png\"/>\n",
    "As I have already mentioned, one star at other end is like an outlier for star class. SVM has a feature to ignore outliers and find the hyper-plane that has maximum margin. Hence, we can say, SVM is robust to outliers.\n",
    "<img src=\"files/svm6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.**Find the hyper-plane to segregate to classes (Scenario-5):** In the scenario below, we can’t have linear hyper-plane between the two classes, so how does SVM classify these two classes? Till now, we have only looked at the linear hyper-plane.\n",
    "<img src=\"files/svm7.png\" /> \n",
    "can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let’s plot the data points on axis x and z:\n",
    "<img src=\"files/svm8.png\" />\n",
    "In above plot, points to consider are:\n",
    "* All values for z would be positive always because z is the squared sum of both x and y\n",
    "* In the original plot, red circles appear close to the origin of x and y axes, leading to lower value of z and star relatively away from the origin result to higher value of z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick:\n",
    "* Kernel Trick is a SVM technique which takes low dimensional input space and transform it to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then find out the process to separate the data based on the labels or outputs you’ve defined.\n",
    "> kernel is essentially a mapping function - one that transforms a given space into some other (usually very high dimensional) space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/svm12.png\"/>\n",
    "\n",
    "You can get some kind of intuition about these concepts by thinking about points and lines in 2D/3D. Here's a very contrived pair of examples...\n",
    "\n",
    "![2D scatter plot][1]\n",
    "\n",
    "This is a plot of a linearly inseparable problem. There is no straight line that can separate the red and blue points.\n",
    "\n",
    "![3D scatter plot][2]\n",
    "\n",
    " [1]: files/svm13.png\n",
    " [2]: files/svm14.png\n",
    " \n",
    "However, if we give each point an extra coordinate (specifically 1 - sqrt(x*x + y*y)... I told you it was contrived), then the problem becomes linearly separable since the red and blue points can be separated by a 2-dimensional plane going through z=0.\n",
    "\n",
    "Hopefully, these examples demonstrate part of the idea behind the kernel trick:\n",
    "\n",
    "*Mapping a problem into a space with a larger number of dimensions makes it more likely that the problem will become linearly separable.*\n",
    "\n",
    "How kernel transform input space : https://www.youtube.com/watch?v=3liCbRZPrZA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the operation of the SVM algorithm is based on finding the hyperplane that gives the largest minimum distance to the training examples. Twice, this distance receives the important name of margin within SVM’s theory. Therefore, the optimal separating hyperplane maximizes the margin of the training data:\n",
    "<img src=\"files/svm17.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function derivation for Linear model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slack variable ξξ is defined as follows (picture from Pattern Recognition and Machine Learning):\n",
    "<img src=\"files/svm15.png\"/>\n",
    "\n",
    "$\\xi_i = 1- y_i(\\omega x_i+b)$ if $x_i$ is on the wrong side of the margin (i.e., $1- y_i(\\omega x_i+b)>0$), \n",
    "$\\xi_i=0$ otherwise. \n",
    "\n",
    "Thus $\\xi_i=max(0, 1- y_i(\\omega x_i+b))\\qquad(1)$.\n",
    "\n",
    "So minimizing the first definition subject to constraint (1) $$ f(\\mathbf{w}) = \\frac{\\left \\| \\mathbf{w} \\right \\|^2}{2} + C(\\sum_{i=1}^{N} \\xi)^k $$ is equivalent to minimizing the second definition (regularizer+hinge loss) $$R(w)+C\\sum max(0, 1- y_i(\\omega x_i+b)).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = ((np.random.rand(100) - 0.5)*6.56).astype(np.float32)\n",
    "y_data = (2*(x_data * 0.1459 + 0.04567 > 0) - 1.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linear kernel line equation is: y=W*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp = W*x_data + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function :\n",
    " $$ J(\\theta) = \\frac{\\left \\| \\mathbf{w} \\right \\|^2}{2} + \\sum_{i=1}^{N} max(0, 1- y_i(\\omega x_i+b)) $$ \n",
    " \n",
    " * Neglecting regularization parameter  C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss =   tf.reduce_mean(tf.maximum(0., (1 - tf.mul(y_data, yp) ))) + tf.reduce_sum(tf.square(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.09)\n",
    "train = optimizer.minimize(loss)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)\n",
    "for i in range(10000):\n",
    "    sess.run(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* Analytic Vidya SVM tutorials https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/\n",
    "* http://stackoverflow.com/questions/1148513/difference-between-a-linear-problem-and-a-non-linear-problem-essence-of-dot-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
