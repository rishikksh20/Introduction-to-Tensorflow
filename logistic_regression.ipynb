{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y=1|\\ x)=\\ \\frac{1}{1-e^{-x*\\beta}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear regression: \\begin{equation*}  h_\\theta(x) = \\theta^Tx \\end{equation*}\n",
    "\\begin{align}  where : \\theta = weight\\ matrix \\ \\ i.e \\ \\ [b_0,b_1,b_2,......,b_n] \\\\\n",
    "\\ \\  \\ \\ X = feature\\ matrix \\ \\ i.e \\ \\ [x_0,x_1,x_2,.......,x_n]\n",
    "\\end{align}\n",
    "                            \n",
    "### \\begin{equation*}  h_\\theta(x) = \\ b_0x_0\\ + \\ b_1x_1\\ + \\ b_2x_2\\ + ......+ \\ b_nx_n \\ = \\sum_{i=0}^{n}b_ix_i \\end{equation*}\n",
    "\\begin{align}\n",
    "where : n = number \\ of \\ features \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression:  \\begin{equation*}  h_\\theta(x) = \\ g(\\  \\theta^Tx )\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "g(z)=\\ \\frac{1}{1-e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}where :\\ z= \\sum_{i=0}^{n}b_ix_i = \\theta^Tx  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"files\\logit.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function: \n",
    "\\begin{equation*}\n",
    "J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m}Cost\\ (\\ h_\\theta(x^i)\\ ,\\ y^i)\\\\\n",
    "\\\\\n",
    "where: m = number \\ of \\ training \\ examples\n",
    "\\end{equation*}\n",
    "<br />\n",
    "<br />\n",
    "\\begin{equation*}Cost\\ (\\ h_\\theta(x)\\ ,\\ y)= \\begin{cases} -\\log(h_\\theta(x)), & \\text{if $y$=1 }\\\\[2ex] -\\log(1-h_\\theta(x)), & \\text{if $y$=0 } \\end{cases} \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cost\\ (\\ h_\\theta(x)\\ ,\\ y)= -y\\ \\log(h_\\theta(x)) - (1-y)\\ \\log(1-h_\\theta(x))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Our Aim to minimize : $$J(\\theta)$$ </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\min_{\\theta}{J(\\theta)}= -\\frac{1}{m}\n",
    "\\left[  \\sum_{i=1}^{m} \\ y_i\\ \\log(h_\\theta(x_i)) + (1-y_i)\\ \\log(1-h_\\theta(x_i))\n",
    "\\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist  = input_data.read_data_sets('data/', one_hot=True)\n",
    "train_x  = mnist.train.images\n",
    "train_y  = mnist.train.labels\n",
    "test_x   = mnist.test.images\n",
    "test_y   = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters of Logistic Regression\n",
    "learning_rate   = 0.01\n",
    "training_epochs = 200\n",
    "batch_size      = 100\n",
    "display_step    = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Graph for Logistic Regression\n",
    "x = tf.placeholder(\"float\", [None, 784], name=\"input\") \n",
    "y = tf.placeholder(\"float\", [None, 10], name=\"output\")  \n",
    "W = tf.Variable(tf.zeros([784, 10]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Softmax \\ Function:\n",
    "g(z)=\\ \\frac{1}{1-e^{-z}} \\\\\n",
    "where \\ z= X*W+b \\\\\n",
    "\\\\\n",
    "X = input \\ matrix \\\\\n",
    "\\\\\n",
    "W = weight \\ matrix \\\\\n",
    "\\\\\n",
    "b = bias\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function is the form of:\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}y^{i}\\log(h_\\theta(x^{i}))+(1-y^{i})\\log(1-h_\\theta(x^{i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum((y * tf.log(pred)) + ((1 - y) * tf.log(1 - pred)), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of correct prediction\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a Session :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    sum_cost = 0.\n",
    "    num_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(num_batch): \n",
    "        randidx  = np.random.randint(train_x.shape[0], size=batch_size)\n",
    "        batch_xs = train_x[randidx, :]\n",
    "        batch_ys = train_y[randidx, :]                \n",
    "        # Fit training using batch data\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optimizer, feed_dict=feeds)\n",
    "        # Compute average loss\n",
    "        sum_cost += sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = sum_cost / num_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        train_acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f train_acc: %.3f\" \n",
    "               % (epoch, training_epochs, avg_cost, train_acc))\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "test_accuracy = sess.run(accuracy, feed_dict={x: test_x, y: test_y})\n",
    "print ((\"Test Accuracy: %.3f\") % (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
